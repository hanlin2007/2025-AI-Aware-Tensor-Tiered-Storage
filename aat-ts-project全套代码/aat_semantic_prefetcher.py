#!/usr/bin/env python3
# aat_semantic_prefetcher.py
"""
AATè¯­ä¹‰é¢„å–å™¨ - å®Œæ•´çœŸå®æ•°æ®ç‰ˆæœ¬
åŸºäºçœŸå®BERTæ¨¡å‹ç»“æ„çš„è¯­ä¹‰é¢„å–ï¼Œä¿®å¤ç»Ÿè®¡é€»è¾‘
"""

import logging
import numpy as np
from collections import defaultdict, deque
import json
import time
import threading

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("AAT-Prefetcher")


class SemanticPrefetcher:
    def __init__(self, storage_manager, history_size=100):
        self.storage_manager = storage_manager
        self.history_size = history_size

        # è®¿é—®å†å²è®°å½•
        self.access_history = deque(maxlen=history_size)
        self.pattern_counts = defaultdict(int)

        # ä¿®å¤é¢„å–å‘½ä¸­ç»Ÿè®¡
        self.prefetch_stats = {
            'prefetched_files': set(),
            'hits': 0,
            'misses': 0,
            'total_prefetches': 0,
            'successful_prefetches': 0
        }

        # å®Œæ•´çš„BERTæ¨¡å‹å±‚é—´ä¾èµ–å…³ç³»
        self.layer_dependencies = {
            'embedding': ['encoder_layer_0'],
            'encoder_layer_0': ['encoder_layer_1'],
            'encoder_layer_1': ['encoder_layer_2'],
            'encoder_layer_2': ['encoder_layer_3'],
            'encoder_layer_3': ['pooler', 'classifier', 'lm_head'],  # å¤šä¸ªå¯èƒ½çš„ä¸‹ä¸€å±‚
            'pooler': ['classifier'],
            'classifier': [],
            'lm_head': [],
            'config': ['embedding']  # é…ç½®é€šå¸¸å…ˆäºåµŒå…¥å±‚è®¿é—®
        }

        # å®Œæ•´çš„æ–‡ä»¶åˆ°å±‚æ˜ å°„
        self.file_to_layer = {
            'embedding.bin': 'embedding',
            'layer0.bin': 'encoder_layer_0',
            'layer1.bin': 'encoder_layer_1',
            'layer2.bin': 'encoder_layer_2',
            'layer3.bin': 'encoder_layer_3',
            'output.bin': 'lm_head',
            'pooler.bin': 'pooler',
            'classifier.bin': 'classifier',
            'config.json': 'config',
            # æ£€æŸ¥ç‚¹æ–‡ä»¶æ˜ å°„
            'checkpoint.ckpt': 'encoder_layer_0',
            'checkpoint_v1.ckpt': 'encoder_layer_1',
            'checkpoint_v2.ckpt': 'encoder_layer_2',
            'checkpoint_v3.ckpt': 'encoder_layer_3',
            'checkpoint_latest.ckpt': 'lm_head'
        }

        # é¢„å–çº¿ç¨‹æ± 
        self.prefetch_threads = []
        self.running = True

        logger.info("è¯­ä¹‰é¢„å–å™¨åˆå§‹åŒ–å®Œæˆ - å®Œæ•´çœŸå®æ•°æ®ç‰ˆæœ¬")

    def record_access(self, filename, operation="read", size=0):
        """è®°å½•æ–‡ä»¶è®¿é—®æ¨¡å¼ - ä¿®å¤é¢„å–å‘½ä¸­æ£€æµ‹"""
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯é¢„å–å‘½ä¸­ï¼ˆåœ¨è®°å½•è®¿é—®ä¹‹å‰ï¼‰
        is_prefetch_hit = filename in self.prefetch_stats['prefetched_files']
        if is_prefetch_hit:
            self.record_prefetch_hit(filename)

        access_record = {
            'filename': filename,
            'operation': operation,
            'size': size,
            'timestamp': time.time(),
            'layer_type': self._classify_layer(filename),
            'prefetch_hit': is_prefetch_hit
        }

        self.access_history.append(access_record)

        # æ›´æ–°æ¨¡å¼ç»Ÿè®¡
        if len(self.access_history) > 1:
            prev_record = self.access_history[-2]
            pattern = f"{prev_record['filename']}->{filename}"
            self.pattern_counts[pattern] += 1

        logger.debug(f"è®°å½•è®¿é—®: {filename} -> {access_record['layer_type']} (é¢„å–å‘½ä¸­: {is_prefetch_hit})")

    def record_prefetch_hit(self, filename):
        """è®°å½•é¢„å–å‘½ä¸­ - ä¿®å¤ç»Ÿè®¡é€»è¾‘"""
        if filename in self.prefetch_stats['prefetched_files']:
            self.prefetch_stats['hits'] += 1
            self.prefetch_stats['prefetched_files'].remove(filename)
            self.prefetch_stats['successful_prefetches'] += 1
            logger.info(f"ğŸ¯ é¢„å–å‘½ä¸­: {filename}")
            return True
        return False

    def _classify_layer(self, filename):
        """æ ¹æ®æ–‡ä»¶ååˆ†ç±»layerç±»å‹ - å®Œæ•´ç‰ˆæœ¬"""
        # ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶æ˜ å°„
        if filename in self.file_to_layer:
            return self.file_to_layer[filename]

        filename_lower = filename.lower()

        if 'embedding' in filename_lower:
            return 'embedding'
        elif 'output' in filename_lower or 'head' in filename_lower:
            return 'lm_head'
        elif 'pooler' in filename_lower:
            return 'pooler'
        elif 'classifier' in filename_lower:
            return 'classifier'
        elif 'config' in filename_lower or 'json' in filename_lower:
            return 'config'
        elif 'layer0' in filename_lower:
            return 'encoder_layer_0'
        elif 'layer1' in filename_lower:
            return 'encoder_layer_1'
        elif 'layer2' in filename_lower:
            return 'encoder_layer_2'
        elif 'layer3' in filename_lower:
            return 'encoder_layer_3'
        elif 'checkpoint' in filename_lower:
            # æ£€æŸ¥ç‚¹æ–‡ä»¶æ˜ å°„åˆ°å¯¹åº”çš„ç¼–ç å™¨å±‚
            if 'v1' in filename_lower:
                return 'encoder_layer_1'
            elif 'v2' in filename_lower:
                return 'encoder_layer_2'
            elif 'v3' in filename_lower:
                return 'encoder_layer_3'
            elif 'latest' in filename_lower:
                return 'lm_head'
            else:
                return 'encoder_layer_0'
        else:
            return 'other'

    def predict_next_layers(self, current_file):
        """é¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½è®¿é—®çš„layer - åŸºäºå®Œæ•´BERTç»“æ„"""
        current_layer = self._classify_layer(current_file)

        # æ–¹æ³•1: åŸºäºé¢„å®šä¹‰çš„ä¾èµ–å…³ç³»
        dependency_based = self.layer_dependencies.get(current_layer, [])

        # å°†å±‚åæ˜ å°„å›æ–‡ä»¶å
        dependency_files = []
        for layer in dependency_based:
            # æŸ¥æ‰¾å¯¹åº”çš„æ–‡ä»¶å
            for file, file_layer in self.file_to_layer.items():
                if file_layer == layer:
                    dependency_files.append(file)
                    break

        # æ–¹æ³•2: åŸºäºå†å²è®¿é—®æ¨¡å¼
        pattern_based = self._get_pattern_based_prediction(current_file)

        # æ–¹æ³•3: åŸºäºå½“å‰åœºæ™¯çš„æ™ºèƒ½é¢„æµ‹
        context_based = self._get_context_based_prediction(current_file, current_layer)

        # åˆå¹¶ç»“æœï¼Œå»é‡
        predicted_files = list(set(dependency_files + pattern_based + context_based))

        logger.info(f"é¢„æµ‹ {current_file}({current_layer}) -> {predicted_files}")
        return predicted_files

    def _get_pattern_based_prediction(self, current_file):
        """åŸºäºå†å²æ¨¡å¼é¢„æµ‹"""
        predictions = []

        # æŸ¥æ‰¾ä»¥å½“å‰æ–‡ä»¶å¼€å¤´çš„æ¨¡å¼
        for pattern, count in self.pattern_counts.items():
            if pattern.startswith(current_file + "->") and count > 1:  # è‡³å°‘å‡ºç°2æ¬¡
                next_file = pattern.split("->")[1]
                predictions.append(next_file)

        return predictions

    def _get_context_based_prediction(self, current_file, current_layer):
        """åŸºäºå½“å‰åœºæ™¯çš„æ™ºèƒ½é¢„æµ‹"""
        context_predictions = []

        # åŸºäºBERTæ¨ç†æµç¨‹çš„æ™ºèƒ½é¢„æµ‹
        if current_layer == 'embedding':
            # åµŒå…¥å±‚åé€šå¸¸è®¿é—®ç¬¬ä¸€ä¸ªç¼–ç å™¨å±‚
            context_predictions.extend(['layer0.bin', 'layer1.bin'])
        elif current_layer.startswith('encoder_layer_'):
            # ç¼–ç å™¨å±‚ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªç¼–ç å™¨å±‚æˆ–è¾“å‡ºå±‚
            layer_num = int(current_layer.split('_')[-1])
            if layer_num < 3:  # å‡è®¾æœ‰4ä¸ªç¼–ç å™¨å±‚ (0-3)
                next_layer = f"layer{layer_num + 1}.bin"
                context_predictions.append(next_layer)
            else:
                # æœ€åä¸€ä¸ªç¼–ç å™¨å±‚åé¢„æµ‹è¾“å‡ºå±‚
                context_predictions.extend(['output.bin', 'pooler.bin'])
        elif current_layer == 'pooler':
            # Pooleråé€šå¸¸è®¿é—®åˆ†ç±»å™¨
            context_predictions.append('classifier.bin')

        return context_predictions

    def prefetch_async(self, current_file):
        """å¼‚æ­¥é¢„å–ç›¸å…³æ–‡ä»¶"""
        if not self.running:
            return

        predicted_files = self.predict_next_layers(current_file)

        for filename in predicted_files:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨é¢„å–ä¸­
            if filename in self.prefetch_stats['prefetched_files']:
                continue

            thread = threading.Thread(
                target=self._prefetch_file,
                args=(filename,)
            )
            thread.daemon = True
            thread.start()
            self.prefetch_threads.append(thread)

    def _prefetch_file(self, filename):
        """é¢„å–å•ä¸ªæ–‡ä»¶"""
        try:
            # æ ‡è®°ä¸ºå·²é¢„å–
            self.prefetch_stats['prefetched_files'].add(filename)
            self.prefetch_stats['total_prefetches'] += 1

            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨çƒ­å±‚
            cached_data = self.storage_manager.get_from_hot_layer(filename)
            if cached_data:
                logger.debug(f"æ–‡ä»¶å·²åœ¨çƒ­å±‚ï¼Œè·³è¿‡é¢„å–: {filename}")
                self.prefetch_stats['successful_prefetches'] += 1
                return

            # ä»çœŸå®æ¨¡å‹è·å–æ•°æ®
            real_data = self.storage_manager.get_real_model_data(filename)
            if real_data:
                # ç¼“å­˜åˆ°çƒ­å±‚
                self.storage_manager.cache_to_hot_layer(filename, real_data)
                logger.info(f"âœ… è¯­ä¹‰é¢„å–å®Œæˆ: {filename} ({len(real_data)} bytes)")
                self.prefetch_stats['successful_prefetches'] += 1
            else:
                # ä»å†·å±‚åŠ è½½
                cold_data = self.storage_manager.get_from_cold_layer(filename)
                if cold_data:
                    # ç¼“å­˜åˆ°çƒ­å±‚
                    self.storage_manager.cache_to_hot_layer(filename, cold_data)
                    logger.info(f"âœ… è¯­ä¹‰é¢„å–å®Œæˆ(å†·å±‚): {filename} ({len(cold_data)} bytes)")
                    self.prefetch_stats['successful_prefetches'] += 1
                else:
                    logger.warning(f"é¢„å–å¤±è´¥ï¼Œæ— æ•°æ®: {filename}")
                    self.prefetch_stats['misses'] += 1
                    # ä»é¢„å–é›†åˆä¸­ç§»é™¤å¤±è´¥çš„æ–‡ä»¶
                    if filename in self.prefetch_stats['prefetched_files']:
                        self.prefetch_stats['prefetched_files'].remove(filename)

        except Exception as e:
            logger.error(f"é¢„å–è¿‡ç¨‹å‡ºé”™ {filename}: {e}")
            self.prefetch_stats['misses'] += 1
            # ä»é¢„å–é›†åˆä¸­ç§»é™¤å¤±è´¥çš„æ–‡ä»¶
            if filename in self.prefetch_stats['prefetched_files']:
                self.prefetch_stats['prefetched_files'].remove(filename)

    def get_access_patterns(self):
        """è·å–è®¿é—®æ¨¡å¼ç»Ÿè®¡"""
        return dict(self.pattern_counts)

    def get_prefetch_stats(self):
        """è·å–é¢„å–ç»Ÿè®¡ - å®Œæ•´ä¿®å¤ç‰ˆæœ¬"""
        total_prefetches = self.prefetch_stats['total_prefetches']
        hits = self.prefetch_stats['hits']
        misses = self.prefetch_stats['misses']
        successful = self.prefetch_stats['successful_prefetches']

        total_attempts = hits + misses
        hit_rate = hits / total_attempts if total_attempts > 0 else 0
        success_rate = successful / total_prefetches if total_prefetches > 0 else 0

        return {
            'prefetch_hits': hits,
            'prefetch_misses': misses,
            'prefetch_hit_rate': hit_rate,
            'total_prefetches': total_prefetches,
            'successful_prefetches': successful,
            'prefetch_success_rate': success_rate,
            'pending_prefetches': len(self.prefetch_stats['prefetched_files']),
            'total_attempts': total_attempts
        }

    def save_patterns(self, filepath):
        """ä¿å­˜å­¦ä¹ åˆ°çš„æ¨¡å¼"""
        try:
            with open(filepath, 'w') as f:
                json.dump({
                    'pattern_counts': dict(self.pattern_counts),
                    'access_history': list(self.access_history),
                    'file_to_layer': self.file_to_layer,
                    'prefetch_stats': self.prefetch_stats,
                    'layer_dependencies': self.layer_dependencies
                }, f, indent=2)
            logger.info(f"æ¨¡å¼å·²ä¿å­˜: {filepath}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å¼å¤±è´¥: {e}")

    def load_patterns(self, filepath):
        """åŠ è½½å·²å­¦ä¹ çš„æ¨¡å¼"""
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
                self.pattern_counts.update(data.get('pattern_counts', {}))
                self.access_history.extend(data.get('access_history', []))
                self.file_to_layer.update(data.get('file_to_layer', {}))
                self.prefetch_stats.update(data.get('prefetch_stats', {
                    'prefetched_files': set(),
                    'hits': 0,
                    'misses': 0,
                    'total_prefetches': 0,
                    'successful_prefetches': 0
                }))
                self.layer_dependencies.update(data.get('layer_dependencies', {}))
            logger.info(f"æ¨¡å¼å·²åŠ è½½: {filepath}")
        except Exception as e:
            logger.warning(f"åŠ è½½æ¨¡å¼å¤±è´¥: {e}")

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.prefetch_stats = {
            'prefetched_files': set(),
            'hits': 0,
            'misses': 0,
            'total_prefetches': 0,
            'successful_prefetches': 0
        }
        self.pattern_counts.clear()
        self.access_history.clear()
        logger.info("é¢„å–ç»Ÿè®¡å’Œæ¨¡å¼å·²é‡ç½®")

    def stop(self):
        """åœæ­¢é¢„å–å™¨"""
        self.running = False
        for thread in self.prefetch_threads:
            thread.join(timeout=1.0)
        logger.info("è¯­ä¹‰é¢„å–å™¨å·²åœæ­¢")


if __name__ == "__main__":
    # æµ‹è¯•é¢„å–å™¨
    class MockStorageManager:
        def get_from_hot_layer(self, filename):
            return None

        def get_real_model_data(self, filename):
            return f"real_data_{filename}".encode()

        def cache_to_hot_layer(self, filename, data):
            return True


    storage_manager = MockStorageManager()
    prefetcher = SemanticPrefetcher(storage_manager)

    # æµ‹è¯•é¢„æµ‹
    test_files = ['embedding.bin', 'layer0.bin', 'layer1.bin']
    for file in test_files:
        predictions = prefetcher.predict_next_layers(file)
        print(f"{file} -> {predictions}")

    # æµ‹è¯•ç»Ÿè®¡
    stats = prefetcher.get_prefetch_stats()
    print(f"é¢„å–ç»Ÿè®¡: {stats}")